{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f098d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers,callbacks\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import random\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c16cd0f",
   "metadata": {},
   "source": [
    "# 1.preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7210f3",
   "metadata": {},
   "source": [
    "### download and unzip dataset from http://www.manythings.org/anki/pes-eng.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99c9519",
   "metadata": {},
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571eab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = \"pes.txt\"\n",
    "with open(text_file,\"r\",encoding=\"utf8\")as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "text_pairs =[]\n",
    "\n",
    "for line in lines:\n",
    "    (english,persian,_) = line.split(\"\\t\")\n",
    "    text_pairs.append((english,\"[start] \"+persian+\" [end]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cec8242",
   "metadata": {},
   "source": [
    "##  split dataset into train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e021125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15* len(text_pairs))\n",
    "num_train_samples = len(text_file) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples+num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples+num_val_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5d40ec",
   "metadata": {},
   "source": [
    "###  Persian punctuation  is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226ec61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_chars = string.punctuation+\"؟\"+\"،\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dd7e01",
   "metadata": {},
   "source": [
    "###  Since [start] , [end] is neccessary in our dataset we should preserve them so we remove \"[\" and \"]\" from punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd06b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_chars = strip_chars.replace(\"[\",\"\")\n",
    "strip_chars = strip_chars.replace(\"]\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fe5d9c",
   "metadata": {},
   "source": [
    "## 1.3  preparing Text vectorization one for english and one for persian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b05327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custome_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase,f\"[{re.escape(strip_chars)}]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b376f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 15000\n",
    "sequence_lenght = 20\n",
    "source_vectorization = TextVectorization(\n",
    "    max_tokens= vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_lenght\n",
    ")\n",
    "\n",
    "target_vectorization = TextVectorization(\n",
    "    max_tokens= vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length = sequence_lenght+1,\n",
    "    standardize = custome_standardization,\n",
    ")\n",
    "\n",
    "train_english_text = [pair[0] for pair in train_pairs]\n",
    "train_persian_text = [pair[1] for pair in train_pairs]\n",
    "source_vectorization.adapt(train_english_text)\n",
    "target_vectorization.adapt(train_persian_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0f81dd",
   "metadata": {},
   "source": [
    "## Preparing datasets for the translation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e8c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "def format_dataset(eng, per):\n",
    "    eng = source_vectorization(eng)\n",
    "    per = target_vectorization(per)\n",
    "    return({\n",
    "        \"english\":eng,\n",
    "        \"persian\":per[:,:-1],\n",
    "    },per[:,1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, per_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    per_texts = list(per_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts,per_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset,num_parallel_calls=8)\n",
    "    return dataset.shuffle(1402).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e522eb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04df0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs,targets in train_ds.take(1):\n",
    "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
    "    print(f\"inputs['persian'].shape: {inputs['persian'].shape}\")\n",
    "    print(f\"targets.shape: {targets.shape}\")\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce93a41",
   "metadata": {},
   "source": [
    "# 2. Sequence to Sequence RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650caf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "latent_dim = 1024\n",
    "source = keras.Input(shape=(None,),dtype=\"int64\",name=\"english\")\n",
    "x = layers.Embedding(vocab_size,embed_dim,mask_zero=True)(source)\n",
    "encoded_source = layers.Bidirectional(layers.GRU(latent_dim),merge_mode=\"sum\")(x)\n",
    "\n",
    "past_target = keras.Input(shape=(None,),dtype=\"int64\",name = \"persian\")\n",
    "x = layers.Embedding(vocab_size,embed_dim,mask_zero=True)(past_target)\n",
    "decoder_gru = layers.GRU(latent_dim,return_sequences=True)\n",
    "x = decoder_gru(x,initial_state=encoded_source)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "target_next_step = layers.Dense(persian_vocab_size,activation=\"softmax\")(x)\n",
    "seq2seq_rnn = keras.Model([source,past_target],target_next_step)\n",
    "seq2seq_rnn.compile(optimizer = \"rmsprop\", loss = \"sparse_categorical_crossentropy\" , metrics =[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ea069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a510174",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "callback = [callbacks.ModelCheckpoint(\"seq2seq.keras\",save_best_only=True)]\n",
    "seq2seq_rnn.fit(train_ds,validati\n",
    "                on_data=val_ds,epochs=50,callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa228b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_rnn = keras.models.load_model(\"seq2seq.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606c0ff4",
   "metadata": {},
   "source": [
    "## 2.1 Create "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5a90ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "persian_vocab = target_vectorization.get_vocabulary()\n",
    "persian_index_lookup = dict(zip(range(len(persian_vocab)),persian_vocab))\n",
    "max_decoded_sentence_lenght=20\n",
    "def decode_sentence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_lenght):\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
    "        next_token_predictions = seq2seq_rnn.predict([tokenized_input_sentence,tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(next_token_predictions[0,i,:])\n",
    "        if sampled_token_index> 3475:\n",
    "            sampled_token = str(sampled_token_index)\n",
    "            sampled_token = \"[start]\"\n",
    "        else:\n",
    "            sampled_token = persian_index_lookup.get(sampled_token_index)\n",
    "        decoded_sentence +=\" \"+str(sampled_token)\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ce97ba",
   "metadata": {},
   "source": [
    "### Test a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f476bcc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_text=\"hello , this is me you are looking for\"\n",
    "decode_sentence(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fa7a8f",
   "metadata": {},
   "source": [
    "# 3. Sequence to Sequence Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913a3c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self,embed_dim,dense_dim,num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads,key_dim=embed_dim)\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(dense_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "    def call(self,inputs,mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:,tf.newaxis,:]\n",
    "        attention_output = self.attention(inputs,inputs,inputs,attention_mask = mask)\n",
    "        proj_input = self.layernorm_1(inputs+attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input+proj_output)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.Update({\n",
    "            \"embed_dim\":self.embed_dim,\n",
    "            \"dense_dim\":self.dense_dim,\n",
    "            \"num_head\":self.num_heads,\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    \n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self,embed_dim,dense_dim,num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention1 = layers.MultiHeadAttention(num_heads=num_heads,key_dim=embed_dim)\n",
    "        self.attention2 = layers.MultiHeadAttention(num_heads=num_heads,key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential([layers.Dense(dense_dim,activation=\"relu\"),\n",
    "                                            layers.Dense(embed_dim),\n",
    "                                           ])\n",
    "        self.layernorm1 = layers.LayerNormalization()\n",
    "        self.layernorm2 = layers.LayerNormalization()\n",
    "        self.layernorm3 = layers.LayerNormalization()\n",
    "        self.support_masking = True\n",
    "    \n",
    "    def get_cadual_attention_mask(self,inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size , sequence_length = input_shape[0],input_shape[1]\n",
    "        i = tf.range(sequence_lenght)[:,tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i>= j , dtype=\"int32\")\n",
    "        mask = tf.reshape(mask,(1,input_shape[1],input_shape[1]))\n",
    "        mult = tf.concat([tf.expand_dims(batch_size,-1),\n",
    "                        tf.constant([1,1],dtype = tf.int32)],axis = 0)\n",
    "        return tf.tile(mask,mult)\n",
    "        \n",
    "    def call(self,inputs,encoder_outputs,mask= None):\n",
    "        cadual_mask = self.get_cadual_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:,tf.newaxis,:],dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, cadual_mask)\n",
    "            \n",
    "        attention_output_1 = self.attention1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=cadual_mask)\n",
    "        \n",
    "        normalization_output_1 = self.layernorm1(inputs+attention_output_1)\n",
    "        attention_output_2 = self.attention2(\n",
    "            query=attention_output_1,\n",
    "            value=encoder_outputs,\n",
    "            key = encoder_outputs,\n",
    "            attention_mask= padding_mask,\n",
    "        )\n",
    "         \n",
    "        dense_proj_input = self.layernorm2(normalization_output_1+attention_output_2)\n",
    "        dense_proj_output = self.dense_proj(dense_proj_input)\n",
    "        return self.layernorm3(dense_proj_input+dense_proj_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = supper().get_config()\n",
    "        config.Update({\n",
    "            \"embed_dim\":self.embed_dim,\n",
    "            \"dense_dim\":self.dense_dim,\n",
    "            \"num_head\":self.num_heads,\n",
    "        })\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c607cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self,sequence_length, input_dim , output_dim , **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(input_dim=input_dim,output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(input_dim=sequence_length,output_dim=output_dim)\n",
    "        \n",
    "        self.sequence_length=sequence_length\n",
    "        self.input_dim=input_dim\n",
    "        self.output_dim=output_dim\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0,limit=length,delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens+embedded_positions\n",
    "    \n",
    "    def compute_mask(self,inputs , mask=None):\n",
    "        return tf.math.not_equal(inputs,0)\n",
    "    \n",
    "    def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "            })\n",
    "            return config    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ed3fc1",
   "metadata": {},
   "source": [
    "## 3.1 Create Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba64b729",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "dense_dim = 1024\n",
    "num_heads = 8\n",
    "\n",
    "encoded_inputs = keras.Input(shape=(None,),dtype=\"int64\" , name=\"english\")\n",
    "x =PositionalEmbedding(sequence_lenght,vocab_size,embed_dim)(encoded_inputs)\n",
    "encoded_outputs = TransformerEncoder(embed_dim, dense_dim,num_heads)(x)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,),dtype=\"int64\",name=\"persian\")\n",
    "x = PositionalEmbedding(sequence_lenght,vocab_size,embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim,dense_dim,num_heads)(x,encoded_outputs)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size,activation=\"softmax\")(x)\n",
    "transformer = keras.Model([encoded_inputs,decoder_inputs],decoder_outputs)\n",
    "\n",
    "transformer.compile(optimizer=\"rmsprop\",loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cf303c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformer.fit(train_ds,validation_data=val_ds,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c7864",
   "metadata": {},
   "outputs": [],
   "source": [
    "persian_vocab = target_vectorization.get_vocabulary()\n",
    "persian_index_lookup = dict(zip(range(len(persian_vocab)),persian_vocab))\n",
    "max_decoded_sentence_lenght=20\n",
    "def decode_sentence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_lenght):\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])[:,:-1]\n",
    "        next_token_predictions = transformer.predict([tokenized_input_sentence,tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(next_token_predictions[0,i,:])\n",
    "        sampled_token = persian_index_lookup.get(sampled_token_index)\n",
    "        decoded_sentence +=\" \"+str(sampled_token)\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bde779",
   "metadata": {},
   "source": [
    "### Test a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ddf906",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text=\"Good morning!\"\n",
    "decode_sentence(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b9c7f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
