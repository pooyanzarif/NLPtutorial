{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib ,shutil , random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"aclImdb\"\n",
    "train_dir = base_dir+\"\\\\train\"\n",
    "val_dir = base_dir+\"\\\\val\"\n",
    "test_dir = base_dir+\"\\\\test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 create validation folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(val_dir)\n",
    "os.makedirs(val_dir+\"\\\\pos\")\n",
    "os.makedirs(val_dir+\"\\\\neg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 move 0.2 of random positive train files to validation folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_files = os.listdir(train_dir+\"\\\\pos\")\n",
    "random.Random(1402).shuffle(pos_files)\n",
    "num_val_samples = int(0.2*len(pos_files))\n",
    "val_pos_files = pos_files[:num_val_samples]\n",
    "for fname in val_pos_files:\n",
    "    shutil.move(train_dir+\"\\\\pos\\\\\"+fname,val_dir+\"\\\\pos\\\\\"+fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 move 0.2 of random negative train files to validation folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_files = os.listdir(train_dir+\"\\\\neg\")\n",
    "random.Random(1402).shuffle(pos_files)\n",
    "num_val_samples = int(0.2*len(pos_files))\n",
    "val_pos_files = pos_files[:num_val_samples]\n",
    "for fname in val_pos_files:\n",
    "    shutil.move(train_dir+\"\\\\neg\\\\\"+fname,val_dir+\"\\\\neg\\\\\"+fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 read text files and make a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_ds = keras.utils.text_dataset_from_directory(train_dir,batch_size=batch_size)\n",
    "val_ds = keras.utils.text_dataset_from_directory(val_dir,batch_size=batch_size)\n",
    "test_ds = keras.utils.text_dataset_from_directory(test_dir,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 1-gram Multi-Hot vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_only_train_ds = train_ds.map(lambda x,y: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "binary_1gram_train_ds = train_ds.map(lambda x,y:(text_vectorization(x),y),num_parallel_calls=8)\n",
    "binary_1gram_val_ds = val_ds.map(lambda x,y:(text_vectorization(x),y),num_parallel_calls=8)\n",
    "binary_1gram_test_ds = test_ds.map(lambda x,y:(text_vectorization(x),y),num_parallel_calls=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(max_tokens=20000 , hidden_dim = 16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim,activation = 'relu')(inputs)\n",
    "    outputs = layers.Dense(1,activation='sigmoid')(x)\n",
    "    model = keras.Model(inputs,outputs)\n",
    "    model.compile(optimizer=\"rmsprop\" , loss= \"binary_crossentropy\" , metrics = [\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks=[keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(binary_1gram_train_ds.cache(),validation_data=binary_1gram_val_ds.cache(),epochs=10 , callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"binary_1gram.keras\")\n",
    "print(f\"Test Acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram Multi-Hot Test Result: 0.887 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bi-gram Molti-Hot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizarion = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "binary_2gram_train_ds = train_ds.map(lambda x,y:(text_vectorization(x),y),num_parallel_calls=8)\n",
    "binary_2gram_val_ds = val_ds.map(lambda x,y:(text_vectorization(x),y),num_parallel_calls=8)\n",
    "binary_2gram_test_ds = test_ds.map(lambda x,y:(text_vectorization(x),y),num_parallel_calls=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "callback=[keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(binary_2gram_train_ds, validation_data=binary_2gram_val_ds,epochs=10 , callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"binary_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram Multi-Hot Test Result: 0.900"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TF-IDF Bigram vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizarion = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens = 20000,\n",
    "    output_mode='tf_idf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "tfidf_2gram_train_ds = train_ds.map(lambda x,y:(text_vectorization(x),y),num_parallel_calls=8)\n",
    "tfidf_2gram_val_ds = val_ds.map(lambda x,y:(text_vectorization(x),y),num_parallel_calls=8)\n",
    "tfidf_2gram_test_ds = test_ds.map(lambda x,y:(text_vectorization(x),y),num_parallel_calls=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "callback=[keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tfidf_2gram_train_ds, validation_data=tfidf_2gram_val_ds,epochs=10 , callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Bigram Test Result: 0.877"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Processing words as a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "int_train_ds = train_ds.map(lambda x,y: (text_vectorization(x),y), num_parallel_calls=8)\n",
    "int_val_ds = val_ds.map(lambda x,y: (text_vectorization(x),y), num_parallel_calls=8)\n",
    "int_test_ds = test_ds.map(lambda x,y: (text_vectorization(x),y), num_parallel_calls=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bidirectional_lstm():\n",
    "    inputs = keras.Input(shape=(None,),dtype=\"int64\")\n",
    "    embedded = tf.one_hot(inputs,depth = max_tokens)\n",
    "    x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1,activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs=inputs,outputs=outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = get_bidirectional_lstm()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "callbacks = [keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",save_best_only=True)]\n",
    "model.fit(int_train_ds,validation_data=int_val_ds,epochs=10, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot bidi-lstm Result: Too slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_bidi_lstm():\n",
    "    inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "    embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
    "    x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = get_embedding_bidi_lstm()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "callbacks = [keras.callbacks.ModelCheckpoint(\"embedding_bidi_lstm.keras\",save_best_only=True)]\n",
    "model.fit(int_train_ds,validation_data=int_val_ds,epochs=10 , callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.load_model(\"embedding_bidi_lstm.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Embedings bidir-lstm  Result: 0.864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_bidi_lstm_mask():\n",
    "    inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "    embedded = layers.Embedding(input_dim=max_tokens, output_dim=256,mask_zero=True)(inputs)\n",
    "    x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = get_embedding_bidi_lstm_mask()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "callbacks = [keras.callbacks.ModelCheckpoint(\"embedding_bidi_lstm_mask.keras\",save_best_only=True)]\n",
    "model.fit(int_train_ds,validation_data=int_val_ds,epochs=10 , callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.load_model(\"embedding_bidi_lstm_mask.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Embedings bidir-lstm with mask  Result: 0.878"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Using pretrained word emdeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 download pretrained GloVe word embeddings (100-dimensional is used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 parsing the GloVe word-embeddings file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = \"glove.6B.100d.txt\"\n",
    "embeddings_index = {}\n",
    "with open(glove_file,encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        word,coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs,\"f\",sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "print(f\"Found {len(embeddings_index)} word vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 preparing the GloVe word-embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## text vectorization based on max_lenght = 600 , max_tokens = 20000 and output_mode = int\n",
    "embeddings_dim = 100\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "vocabulary = text_vectorization.get_vocabulary(include_special_tokens=False)\n",
    "word_index = dict(zip(vocabulary,range(len(vocabulary))))\n",
    "embeddings_matrix = np.zeros((max_tokens,embeddings_dim))\n",
    "\n",
    "for i , word in enumerate(vocabulary):\n",
    "    if i < max_tokens:\n",
    "        embeddings_vector=embeddings_index.get(word)\n",
    "    if embeddings_vector is not None:\n",
    "        embeddings_matrix[i]=embeddings_index.get(word)\n",
    "        \n",
    "embeddings_layer = layers.Embedding(\n",
    "    max_tokens,embeddings_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embeddings_matrix),\n",
    "    trainable = False,\n",
    "    mask_zero=True\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_GloVe_embeding_model():\n",
    "    inputs = keras.Input(shape=(None,),dtype=\"int64\")\n",
    "    embedded = embeddings_layer(inputs)\n",
    "    x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs,outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_GloVe_embeding_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\", save_best_only=True)]\n",
    "model.fit(int_train_ds,validation_data=int_val_ds,epochs=10,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  GloVe Embedings bidir-lstm with mask  Result: 0.771"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self,embed_dim,dense_dim,num_heads,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads,key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "        [layers.Dense(dense_dim,activation=\"relu\"),\n",
    "         layers.Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization()\n",
    "        self.layernorm2 = layers.LayerNormalization()\n",
    "        \n",
    "    def call(self,inputs,mask=None):\n",
    "        if mask is not None:\n",
    "            mask =mask[:,tf.newaxis,:]\n",
    "        attention_output = self.attention(inputs,inputs,attention_mask=mask)\n",
    "        proj_input =self.layernorm1(inputs+attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm2(proj_input+proj_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\":self.embed_dim,\n",
    "            \"num_heads\":self.num_heads,\n",
    "            \"dense_dim\":self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self,sequence_length, input_dim , output_dim , **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(input_dim=input_dim,output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(input_dim=sequence_length,output_dim=output_dim)\n",
    "        \n",
    "        self.sequence_length=sequence_length\n",
    "        self.input_dim=input_dim\n",
    "        self.output_dim=output_dim\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0,limit=length,delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens+embedded_positions\n",
    "    \n",
    "    def compute_mask(self,inputs , mask=None):\n",
    "        return tf.math.not_equal(inputs,0)\n",
    "    \n",
    "    def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "            })\n",
    "            return config    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "sequence_length = 600\n",
    "embed_dim = 256\n",
    "num_head = 2\n",
    "dense_dim = 32\n",
    "inputs = keras.Input(shape=(None,),dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length,vocab_size,embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim,dense_dim,num_head)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1,activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs,outputs)\n",
    "model.compile(optimizer=\"rmsprop\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\",save_best_only=True)]\n",
    "\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"transformer_encoder.keras\",custom_objects={\"TransformerEncoder\": TransformerEncoder, \"PositionalEmbedding\":PositionalEmbedding})\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Transformer Result: 0.884"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
